diff --git a/nmtpy/models/attention.py b/nmtpy/models/attention.py
index c987d3b..7009ace 100644
--- a/nmtpy/models/attention.py
+++ b/nmtpy/models/attention.py
@@ -206,8 +206,10 @@ class Model(BaseModel):
         y_flat = y.flatten()
         y_flat_idx = tensor.arange(y_flat.shape[0]) * self.n_words_trg + y_flat
 
+        # y: max_seq_len x batch_size
         cost = log_probs.flatten()[y_flat_idx]
         cost = cost.reshape([y.shape[0], y.shape[1]])
+        # Sum costs per sample ignoring the contribution of padded positions
         cost = (cost * y_mask).sum(0)
 
         self.f_log_probs = theano.function(self.inputs.values(),
@@ -220,7 +222,8 @@ class Model(BaseModel):
         self.y_mask = y_mask
         self.alphas = alphas
 
-        return cost.mean()
+        # Normalize cost wrt batch and wrt word
+        return (cost / y_mask.sum(0)).mean()
 
     def add_alpha_regularizer(self, cost, alpha_c):
         alpha_c = theano.shared(alpha_c.astype(FLOAT), name='alpha_c')

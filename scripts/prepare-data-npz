#!/usr/bin/env python
import argparse
import numpy as np
import h5py
import os
import glob
import random

from itertools import izip

def get_imgfeats_array(fname):
    print "Loading %s" % fname
    if fname.endswith(".npy"):
        return np.load(fname)
    elif "hdf5" in fname:
        # Read HDF5 file
        hf = h5py.File(fname, 'r')
        feats = hf['feats']
        data = np.empty((feats.shape[0], feats.shape[1]), dtype=np.float32)
        feats.read_direct(data)
        hf.close()
        return data

def get_sentence_tokens(fname):
    sents = []
    with open(fname, 'r') as f:
        for sent in f:
            # Strip and split from whitespace
            sents.append(sent.strip().split(" "))
    return sents

def get_split_list(srcs, trgs, start_idx, image_list, maxlen=50, ratio=3.):
    assert len(srcs) == len(trgs)

    split_list = []
    for idx, (s, t) in enumerate(izip(srcs, trgs)):
        sc = len(s)
        tc = len(t)
        r = float(sc) / tc
        # Cleanup sentences > maxlen and src-trg-src ratio > 3
        if sc > maxlen or tc > maxlen or r > ratio or (1/r) > ratio:
            continue

        img_id = start_idx + idx
        split_list.append((s, t, img_id, image_list[img_id]))

    print "%d/%d sentence pairs kept (maxlen=%d, ratio=%d)" % (len(split_list), len(srcs), maxlen, ratio)
    return split_list

def get_multi_splits(srcfile, trgfile, start_idx, image_list, maxlen=50, ratio=3.):
    src_files = glob.glob(srcfile)
    trg_files = glob.glob(trgfile)
    assert len(src_files) == len(trg_files)
    n_splits = len(src_files)
    print "Found %d parallel corpora" % n_splits

    src_splits = []
    trg_splits = []
    for i in range(n_splits):
        src_splits.append(get_sentence_tokens(src_files[i]))
        trg_splits.append(get_sentence_tokens(trg_files[i]))
 
    pairs = []
    for i, ss in enumerate(src_splits, 1):
        for j, tt in enumerate(trg_splits, 1):
            print i, j, 
            pairs.extend(get_split_list(ss, tt, start_idx, image_list, maxlen, ratio))
    return pairs

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='prepare-pkl')

    parser.add_argument('-i', '--image-feats'   , type=str, required=True, help="Path to image feats file.")
    parser.add_argument('-f', '--image-list'    , type=str, required=True, help="Ordered list of images for the image-feats file.")
    parser.add_argument('-o', '--output'        , type=str, required=True, help="Output file.")
    parser.add_argument('-t', '--srcfile'       , type=str, required=True, help="Source training sentences. (can be a wildcard pattern)")
    parser.add_argument('-T', '--trgfile'       , type=str, required=True, help="Target training sentences. (can be a wildcard pattern)")

    parser.add_argument('-v', '--validsrcfile'  , type=str, required=False,help="Source dev sentences. (can be a wildcard pattern)")
    parser.add_argument('-V', '--validtrgfile'  , type=str, required=False,help="Target dev sentences. (can be a wildcard pattern)")
    parser.add_argument('-e', '--testsrcfile'   , type=str, required=False,help="Source test sentences. (can be a wildcard pattern)")
    parser.add_argument('-E', '--testtrgfile'   , type=str, required=False,help="Target test sentences. (can be a wildcard pattern)")

    args = parser.parse_args()

    for k,v in args.__dict__.iteritems():
        if k.endswith("file") and v:
            args.__dict__[k] = os.path.expanduser(v)

    # This is the image features file for all splits
    img_feats = get_imgfeats_array(args.image_feats)
    print "Image features: %s" % str(img_feats.shape)

    # Let's also keep image file names
    with open(args.image_list, 'r') as f:
        image_list = [i for i in f.read().strip().split("\n")]

    # We should have same amount of images as the feats array's first dimension
    assert len(image_list) == img_feats.shape[0]

    #######################
    # Create the dictionary
    #######################
    data = {}

    data['feats_file'] = os.path.basename(args.image_feats)

    # Actual data split boundaries
    trains = slice(0, 29000)
    valids = slice(29000, 30014)
    tests  = slice(30014, img_feats.shape[0])

    data['feats'] = img_feats

    print "Reading training sentences"
    data['train'] = get_multi_splits(args.srcfile, args.trgfile, trains.start, image_list)
    print "%d training sentences" % len(data['train'])

    if args.validsrcfile and args.validtrgfile:
        print "Reading validation sentences"
        data['valid'] = get_multi_splits(args.validsrcfile, args.validtrgfile, valids.start, image_list)
        print "%d validation sentences" % len(data['valid'])

    if args.testsrcfile and args.testtrgfile:
        print "Reading test sentences"
        data['test'] = get_multi_splits(args.testsrcfile, args.testtrgfile, tests.start, image_list)
        print "%d test sentences" % len(data['test'])

    # Save the data
    np.savez(args.output, **data)

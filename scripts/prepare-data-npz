#!/usr/bin/env python
import argparse
import numpy as np
import h5py
import os
import random

from itertools import izip

def get_imgfeats_array(fname):
    print "Loading %s" % fname
    if fname.endswith(".npy"):
        return np.load(fname)
    elif "hdf5" in fname:
        # Read HDF5 file
        hf = h5py.File(fname, 'r')
        feats = hf['feats']
        data = np.empty((feats.shape[0], feats.shape[1]), dtype=np.float32)
        feats.read_direct(data)
        hf.close()
        return data

def get_sentence_tokens(fname):
    sents = []
    with open(fname, 'r') as f:
        for sent in f:
            # Strip and split from whitespace
            sents.append(sent.strip().split(" "))

    return sents

def get_split_list(srcfile, trgfile, start_idx, image_list):
    split_list = []
    src_sents = get_sentence_tokens(srcfile)
    trg_sents = get_sentence_tokens(trgfile)
    assert len(src_sents) == len(trg_sents)

    # NOTE: Order matters!
    for idx, (s, t) in enumerate(izip(src_sents, trg_sents)):
        img_id = start_idx + idx
        split_list.append((s, t, img_id, image_list[img_id]))

    return split_list

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='prepare-pkl')

    parser.add_argument('-i', '--image-feats'   , type=str, required=True, help="Path to image feats file.")
    parser.add_argument('-n', '--image-idx'     , type=str, required=True, help="Image idxs for each sentence pair.")
    parser.add_argument('-f', '--image-list'    , type=str, required=True, help="Ordered list of images for the image-feats file.")
    parser.add_argument('-o', '--output'        , type=str, required=True, help="Output file.")
    parser.add_argument('-t', '--srcfile'       , type=str, required=True, help="Source training sentences.")
    parser.add_argument('-T', '--trgfile'       , type=str, required=True, help="Target training sentences.")

    parser.add_argument('-v', '--validsrcfile'  , type=str, required=False,help="Source dev sentences.")
    parser.add_argument('-V', '--validtrgfile'  , type=str, required=False,help="Target dev sentences.")
    parser.add_argument('-e', '--testsrcfile'   , type=str, required=False,help="Source test sentences.")
    parser.add_argument('-E', '--testtrgfile'   , type=str, required=False,help="Target test sentences.")

    args = parser.parse_args()

    img_feats = get_imgfeats_array(args.image_feats)
    print "Image features: %s" % str(img_feats.shape)

    # Read source and target sentences
    print "Reading source and target training sentences..."
    src_sents = get_sentence_tokens(args.srcfile)
    trg_sents = get_sentence_tokens(args.trgfile)

    assert len(src_sents) == len(trg_sents)
    print "Number of sentence pairs: %d" % len(src_sents)

    # This is for keeping track of which sentence pair belongs to which
    # image. This is important after filtering out some sentences
    # and the order is messed up.
    # In Flickr30k en->de task, 2 sentences are buggy. When we remove
    # them we're left with 28998 sentences with a broken order.
    with open(args.image_idx, 'r') as f:
        # NOTE: Lines retained files of clean-corpus starts from 1!
        image_idxs = [int(i)-1 for i in f.read().strip().split("\n")]
    print "Found %d image idxs in %s" % (len(image_idxs), args.image_idx)

    # Let's also keep image file names
    with open(args.image_list, 'r') as f:
        image_list = [i for i in f.read().strip().split("\n")]

    # We should have same amount of images as the feats array's first dimension
    assert len(image_list) == img_feats.shape[0]

    #######################
    # Create the dictionary
    #######################
    data = {}

    data['feats_file'] = os.path.basename(args.image_feats)

    # Actual data split boundaries
    trains = slice(0, 29000)
    valids = slice(29000, 30014)
    tests  = slice(30014, img_feats.shape[0])

    data['feats'] = img_feats
    data['train'] = {}

    # Let's do this a list of tuples (srcsent, trgsent, imgid, imgfilename)

    print "Reading training sentences"
    data['train']['sents'] = get_split_list(args.srcfile, args.trgfile, trains.start, image_list)
    print "%d training sentences" % len(data['train']['sents'])

    if args.validsrcfile and args.validtrgfile:
        data['valid'] = {}
        print "Reading validation sentences"
        data['valid']['sents'] = get_split_list(args.validsrcfile, args.validtrgfile, valids.start, image_list)
        print "%d validation sentences" % len(data['valid']['sents'])

    if args.testsrcfile and args.testtrgfile:
        data['test']  = {}
        print "Reading test sentences"
        data['test']['sents'] = get_split_list(args.testsrcfile, args.testtrgfile, tests.start, image_list)
        print "%d test sentences" % len(data['test']['sents'])

    # Save the data
    np.savez(args.output, **data)

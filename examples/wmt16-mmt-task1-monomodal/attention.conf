[training]
model_type: attention
patience: 30
max_epochs: 100
valid_freq: 1000
valid_metric: meteor
decay_c: 1e-5
clip_c: 5
seed: 1235

[model]
tied_trg_emb: True
layer_norm: True
shuffle_mode: trglen

emb_dropout: 0.2
ctx_dropout: 0.4
out_dropout: 0.4

rnn_dim: 100
embedding_dim: 100

weight_init: xavier
batch_size: 32
optimizer: adam
lrate: 0.0002

filter: bpe

n_words_src: 0
n_words_trg: 0

save_path: ~/models/nmtpy

[model.dicts]
src: ~/data/task1/bpe20000/train.norm.max50.tok.lc.bpe.en.pkl
trg: ~/data/task1/bpe20000/train.norm.max50.tok.lc.bpe.de.pkl

[model.data]
train_src     : ~/data/task1/bpe20000/train.norm.max50.tok.lc.bpe.en
train_trg     : ~/data/task1/bpe20000/train.norm.max50.tok.lc.bpe.de
valid_src     : ~/data/task1/bpe20000/valid.norm.max50.tok.lc.bpe.en
# This is for loss computation in training. This is the actual reference file
valid_trg     : ~/data/task1/bpe20000/valid.norm.max50.tok.lc.bpe.de
# This is the same file with BPE reverted to be used by nmt-translate for metric computation
valid_trg_orig: ~/data/task1/val.norm.lc.tok.de

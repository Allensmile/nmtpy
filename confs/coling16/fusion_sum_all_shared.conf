# Main .py file which will be used for the model
model-type: fusion_sum

conv-dim: 1024

# Word embedding vector dimension
trg-emb-dim: 620
src-emb-dim: 620

# Decoder RNN's hidden layer dimension
rnn-dim: 1000

# adadelta, adam, sgd or rmsprop
optimizer: adam

# 0: no, otherwise dropout probability
dropout: 0.0

# 0: no, otherwise weight decay factor
decay-c: 0.00001

# -1: no, otherwise maximum gradient norm
clip-c: 5

# 0: no, otherwise alpha regularization factor
alpha-c: 0.0

# Weight initialization scheme
weight-init: xavier

# batch size
batch-size: 32

# how much validation period will we wait to do early stopping
patience: 10

# Maximum number of epochs before stopping training
max-epochs: 100

# validation frequency in terms of minibatch updates
# 0 means at end of epochs
valid-freq: 0

# Use BLEU as validation metric
valid-metric: bleu

# 0: use all vocabulary, otherwise upper limit as integer
n-words-trg: 10000
n-words-src: 0

# Where to save model params, weights and training log file
model-path: ~/wmt16/coling2016

data: { \
  "train_img" : '/lium/trad4a/wmt/2016/data/resnet-feats/flickr30k_ResNets50_blck4_train.npy', \
  "valid_img" : '/lium/trad4a/wmt/2016/data/resnet-feats/flickr30k_ResNets50_blck4_val.npy', \
  "train_src" : '~/wmt16/data/task2/cross-product-min3-max50-minvocab5-train-680k/flickr_30k_align.train.pkl', \
  "valid_src" : '~/wmt16/data/task2/cross-product-min3-max50-minvocab5-train-680k/flickr_30k_align.valid.pkl', \
  "valid_trg" : '~/wmt16/data/task2/cross-product-min3-max50-minvocab5-train-680k/valid.*.tok.lc.nopunct.de', \
  }

# Dictionaries
dicts: { \
  "src" : '~/wmt16/data/task2/cross-product-min3-max50-minvocab5-train-680k/train_src.pkl', \
  "trg" : '~/wmt16/data/task2/cross-product-min3-max50-minvocab5-train-680k/train_trg.pkl', \
  }

# auto: select the first available, otherwise integer
device-id: auto

# reload existing model and continue training
restore: False

# profile theano ops
profile: False

# Main .py file which will be used for the model
model-type: attention

norm-cost: True
tied-trg-emb: True
shuffle-mode: trglen
layer-norm: True


# Embedding vector dimension
embedding-dim: 200

# RNN's hidden layer dimension
rnn-dim: 400

# adadelta, adam, sgd or rmsprop
optimizer: adam

# Learning rate (only for SGD)
lrate: 0.0004

# 0: no, otherwise weight decay factor
decay-c: 0.0

# -1: no, otherwise maximum gradient norm
clip-c: 5

# 0: no, otherwise alpha regularization factor
alpha-c: 0.

# batch size
batch-size: 32

# how much validation period will we wait
# to do early stopping
patience: 50

# Maximum number of epochs before stopping training
# Corpus takes ~50K iterations per epoch for batchsize = 80
# Corpus takes ~122K iterations per epoch for batchsize = 32
max-epochs: 200

# Stop after 5M iterations
max-iteration: 5000000

# validation frequency in terms of minibatch updates
valid-freq: 10000

# Use BLEU as additional validation metric
valid-metric: bleu

# Start validation after 1st epoch is finished
valid-start: 2

# Let's do early stopping with a reduced beam size for speed
beam-size: 3

# Weight init is Xavier
weight-init: xavier

# Postprocessing filter: BPE
filter: bpe

# 0: use all vocabulary, otherwise upper limit as integer
n-words-src: 0
n-words-trg: 0

# Where to save model params, weights and training log file
model-path: ~/models/shortpaper/uedin

# Dictionaries if necessary
dicts: { \
  "src":  "~/data/wmt16-news/en-de/rico_tokenized/data/corpus.tok.clean.bpe.en.pkl", \
  "trg":  "~/data/wmt16-news/en-de/rico_tokenized/data/corpus.tok.clean.bpe.de.pkl", \
  }

data: { \
  "train_src"     : '~/data/wmt16-news/en-de/rico_tokenized/data/corpus.tok.clean.bpe.en', \
  "train_trg"     : '~/data/wmt16-news/en-de/rico_tokenized/data/corpus.tok.clean.bpe.de', \
  "valid_src"     : '~/data/wmt16-news/en-de/rico_tokenized/data/newstest2013.tok.bpe.en', \
  # This is for loss computation in training, still with BPE tokens.
  "valid_trg"     : '~/data/wmt16-news/en-de/rico_tokenized/data/newstest2013.tok.bpe.de', \
  # This is the same file with BPE reverted to be used by nmt-translate for metric computation
  "valid_trg_orig": '~/data/wmt16-news/en-de/rico_tokenized/data/newstest2013.tok.de', \
  }

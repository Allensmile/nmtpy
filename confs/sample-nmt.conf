# A sample configuration file for nmtpy
# -------------------------------------
# This file will be included in your final model checkpoint so that
# it is not needed once the training is over.
#
# This file is parsed by the Config class in nmtpy/config.py.
#
# Description:
# ------------
#  - key, value pairs are separated with ':'
#  - You can use tilda (~) for paths to refer to your $HOME
#  - All options given here will be first consumed by nmt-train and what is left
#    will be passed to your model's __init__() method.
#    Example:
#     valid_freq is an nmt-train option while rnn-dim is expected from your model's __init__()

###############################
# Options expected by nmt-train
###############################
# model-type defines your model/architecture:
# You should have a file called mynmt.py inside nmtpy/models which derives from BaseModel.
model-type: mynmt

# Weight initialization scheme for all layers
# "xavier", "he" or a floating point number (Ex: 0.01) for initialization
# with normal distribution using that number as the "scale"
weight-init: xavier

# batch size
batch-size: 32

# adadelta, adam, sgd or rmsprop
optimizer: adam

# initial learning rate
lrate: 0.0002

# L2 weight decay factor. 0 means disabled
decay-c: 0.

# Gradient norm clipping. -1 means disabled
# A value of 1 or 5 is generally used throughout the literature.
clip-c: 5.

# Attention weight regularization. 0 means disabled
# This was available in dl4mt-tutorial. Not tested at all.
alpha-c: 0.

# How much patience (validations) to stop training?
patience: 10

# Maximum number of epochs before stopping training
# Maximum number of iterations before stopping training
# Whichever comes first, define the stopping point.
max-epochs: 100
max-iteration: 5000000

# Start validation after 1st epoch is finished
valid-start: 1

# validation frequency in terms of minibatch updates
valid-freq: 1000

# Use BLEU for early-stopping metric (Supports bleu/meteor)
valid-metric: bleu

# Beam size used during early-stopping decoding
# You may want to keep this low for training
beam-size: 3

# Postprocessing filter for produced hypotheses (bpe,compound)
# Remove preprocessing related characters/words (@@ for BPE) before
# evaluating the performance with BLEU/METEOR and before writing the
# translations into a file using nmt-translate.
filter: bpe

# Size of the most-frequent word shortlist for vocabularies
# 0: Use all vocabulary from the dictionaries
# N: Use the most frequent N items and map the rest to <unk> 
n-words-src: 0
n-words-trg: 0

######################################
# Other options expected by your model
######################################
# This is passed by your model to your iterator to define shuffling method
shuffle-mode: trglen

# If you had incorporated dropout inside your model, you can pass the dropout rates
# using these parameters.
emb-dropout: 0.
ctx-dropout: 0.
out-dropout: 0.

# Embedding dimensionality
embedding-dim: 500

# RNN's hidden layer dimension
rnn-dim: 1024

##################################
# Storage and data related options
##################################

# Training log and model checkpoints will be saved in a directory of the form:
# "model-path/model-type"
# For this conf, model-type was mynmt, so the final storage folder will be:
# ~/nmtpy-models/iwslt15-en-fr/mynmt
model-path: ~/nmtpy-models/iwslt15-en-fr

# Paths to source and target dictionaries created using nmt-build-dict utility
dicts: { \
  "src":  "~/data/iwslt15/en-fr/corpus.parallel.tok.max50.tc.bpe.en.pkl", \
  "trg":  "~/data/iwslt15/en-fr/corpus.parallel.tok.max50.tc.bpe.fr.pkl", \
  }

# Training and validation data
data: { \
  "train_src"     : '~/data/iwslt15/en-fr/corpus.parallel.tok.max50.tc.bpe.en', \
  "train_trg"     : '~/data/iwslt15/en-fr/corpus.parallel.tok.max50.tc.bpe.fr', \
  "valid_src"     : '~/data/iwslt15/en-fr/valid.tok.max50.tc.bpe.en', \
  # valid_trg is for loss/perplexity computation in training, still with BPE tokens.
  "valid_trg"     : '~/data/iwslt15/en-fr/valid.tok.max50.tc.bpe.fr', \
  # This is the same file as valid_trg with BPE reverted to be used by nmt-translate
  # for BLEU/METEOR computation.
  "valid_trg_orig": '~/data/wmt16-news/en-de/train_bpe/newstest2013.tok.tc.de', \
  }

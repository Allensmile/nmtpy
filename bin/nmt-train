#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
import logging
import argparse
import importlib
import platform

from nmtpy.config import Config
from nmtpy.logger import Logger
from nmtpy.sysutils import setup_train_args, get_device
from nmtpy.nmtutils import DotDict, get_param_dict
from nmtpy.mainloop import MainLoop

# Ensure cleaning up temp files and processes
import nmtpy.cleanup as cleanup
cleanup.register_handler()

# Bring default values for arguments which are not set
from nmtpy.defaults import DEFAULTS, TRAIN_DEFAULTS
all_defaults = dict(DEFAULTS)
all_defaults.update(TRAIN_DEFAULTS)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='nmt-train')

    parser.add_argument('-a', '--alpha-c'       , type=float, help="Alpha regularization factor")
    parser.add_argument('-g', '--clip-c'        , type=float, help="Gradient clipping factor")
    parser.add_argument('-w', '--decay-c'       , type=float, help="Weight decay factor")
    parser.add_argument('-c', '--config'        , type=str  , help="Path to model configuration file", required=True)
    parser.add_argument('-T', '--model-type'    , type=str  , help="Override the model type given in the configuration file")
    parser.add_argument('-D', '--device-id'     , type=str  , help="Device to use")
    parser.add_argument('-M', '--valid-metric'  , type=str  , help="Validation metric: px, bleu, meteor")
    parser.add_argument('-j', '--njobs'         , type=str  , help="# of CPU jobs for beam-search")
    parser.add_argument('-l', '--lrate'         , type=str  , help="Initial learning rate")
    parser.add_argument('-i', '--init'          , type=str  , help="Init parameters from a trained one")
    parser.add_argument('-o', '--optimizer'     , type=str  , help="Optimizer: sgd, adadelta, rmsprop, adam")
    parser.add_argument('-s', '--suffix'        , type=str  , help="Optional suffix to append to model name")
    parser.add_argument('-R', '--seed'          , type=int  , help="Random seed")
    parser.add_argument('-V', '--valid-freq'    , type=int  , help="Validation frequency in terms of updates")
    parser.add_argument('-b', '--batch-size'    , type=int  , help="Training batch size")
    parser.add_argument('-d', '--debug'         , action='store_true', help="Enable debugging outputs (Warning: very verbose)")
    parser.add_argument('-e', '--extra'         , nargs='*' , help="Extra key:value pairs as in conf to override them")

    ##############################
    # Parse command-line arguments
    cmd_args = parser.parse_args()

    # Parse configuration file for arguments
    args = Config(cmd_args.config).get()

    # Create a folder named as conf file
    args['model_path_suffix'] = os.path.splitext(os.path.basename(cmd_args.config))[0]
    extra_params = cmd_args.extra
    del cmd_args.__dict__['config']
    del cmd_args.__dict__['extra']

    # Override .conf arguments from the ones given in cmdline
    args.update([(k,v) for k,v in cmd_args.__dict__.items() if v is not None])

    for k,v in all_defaults.iteritems():
        if k not in args:
            args[k] = v

    if extra_params:
        for opt in extra_params:
            key, value = opt.split(":")
            key = key.replace('-', '_')
            args[key] = eval(value, {}, {})

    # Setup arguments
    args, log_file = setup_train_args(args)
    # Pass parameters not in TRAIN_DEFAULTS as they are for this mainloop
    model_args = dict([(k,v) for k,v in args.iteritems() if k not in TRAIN_DEFAULTS])
    train_args = dict([(k,v) for k,v in args.iteritems() if k in TRAIN_DEFAULTS])
    train_args = DotDict(train_args)

    # Start logging module (both to terminal and to file)
    Logger.set_file(log_file)
    log = Logger._logger

    # Set device for Theano
    if 'THEANO_FLAGS' not in os.environ:
        args.device_id = get_device(args.device_id)
        os.environ['THEANO_FLAGS'] = "device=%s" % args.device_id

    log.info("THEANO_FLAGS = %s" % os.environ['THEANO_FLAGS'])

    # Import theano
    import theano
    import numpy as np
    log.info("Using device: %s (on machine %s)" % (args.device_id, platform.node()))

    # Set numpy random seed before everything else
    if args.seed != 0:
        np.random.seed(args.seed)

    # Import the model
    try:
       Model = importlib.import_module("nmtpy.models.%s" % args.model_type).Model
    except ImportError as e:
        log.error("Error while importing %s" % args.model_type)
        log.error(e)
        sys.exit(1)

    # Print parameter summary
    log.info("Model options:")
    log.info("--------------")
    for p in sorted(args.keys()):
        log.info(" %20s -> %s" % (p, args[p]))

    # Create model object
    model = Model(seed=args.seed, logger=log, **model_args)

    # Initialize parameters
    log.info("Initializing parameters")
    model.init_params()

    # Load pre-trained model weights if requested
    init_params = None
    if "init" in args:
        log.info('Overriding parameters from pre-trained model')
        init_params = get_param_dict(args.init)

    # Create theano shared variables
    log.info('Creating shared variables')
    model.init_shared_variables(_from=init_params)

    # Print number of parameters
    log.info("Number of parameters: %s" % model.get_nb_params())

    # Load data
    log.info("Loading data")
    model.load_data()

    # Dump model information
    model.info()

    # Build the model
    log.info("Building model")
    data_loss = model.build()

    log.info("Input tensor order: ")
    log.info(model.inputs.values())

    if args.sample_freq > 0:
        log.info('Building sampler')
        model.build_sampler()

    # Compute regularized loss
    reg_loss = []
    if args.decay_c > 0:
        reg_loss.append(model.get_l2_weight_decay(args.decay_c))
    if args.alpha_c > 0:
        reg_loss.append(model.get_alpha_regularizer(args.alpha_c))

    reg_loss = sum(reg_loss) if len(reg_loss) > 0 else None

    # Build optimizer
    log.info('Building optimizer %s (initial lr=%.5f)' % (args.optimizer, model.lrate))
    model.build_optimizer(data_loss, reg_loss, args.clip_c, debug=args.debug)

    # Save graph
    if args.debug:
        theano.printing.debugprint(model.train_batch, file=open('%s.graph' % log_file.replace(".log", ""), 'w'))

    # Reseed to retain the order of shuffle operations
    np.random.seed(args.seed)

    # Create mainloop
    MainLoop(model, log, train_args).run()

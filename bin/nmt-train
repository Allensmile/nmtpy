#!/usr/bin/env python

import os
import sys
import time
import math
import shutil
import logging
import argparse
import platform
import importlib
import cPickle as pkl

import numpy as np

# Our classes
from nmtpy.config import Config
from nmtpy.logger import Logger
from nmtpy.typedef import INT
from nmtpy.sysutils import *
from nmtpy.nmtutils import unzip
import nmtpy.cleanup as cleanup

cleanup.register_handler()

DEFAULTS = {
        'weight_init':        0.01,           # Scale of the uniform distribution for weight initialization.
                                              # Can be a float, "xavier" or "he".
        'batch_size':         32,             # Training batch size
        'lrate':              0.0001,         # lrate for SGD
        'optimizer':          'adadelta',     # adadelta, sgd, rmsprop, adam
        'dropout':            0,              # dropout ratio (0: disabled)
        'profile':            False,          # Profile Theano
        'nanguard':           False,          #
        }

TRAIN_DEFAULTS = {
        'decay_c':            0.0005,         # L2 penalty factor
        'clip_c':             0.,             # Clip gradients above clip_c
        'alpha_c':            0.,             # Alpha regularization for attentional models
        'seed':               1234,           # RNG seed
        'suffix':             "",             # Model suffix
        'save_iter':          False,          # Save each best valid weights to separate file
        'restore':            False,
        'device_id':          'auto',         #
        'patience':           10,             # Early stopping patience
        'max_epochs':         100,            # Max number of epochs to train
        'max_iteration':      1e6,            # Max number of updates to train
        'valid_start':        1,              # Epoch which validation will start
        'valid_freq':         0,              # 0: End of epochs
        'valid_metric':       'bleu',         # bleu, px, meteor
        #'valid_freq_decay':   10,             # Halve validation frequency after epoch 10 for once if the perf doesn't improve
        'sample_freq':        0,              # Sampling frequency during training (0: disabled)
        'njobs':              8,              # # of parallel CPU tasks to do beam-search
        'decoder_mode':       "beamsearch",
        }

def setup(args):
    # Check METEOR path
    if args.valid_metric == "meteor":
        if "meteor_path" in args:
            os.environ['METEOR_JAR'] = args['meteor_path']
        else:
            raise Exception("You need to provide 'meteor-path' in your configuration.")

    # Find out dimensional information
    dim_str = ""
    for k in sorted(args):
        if k.endswith("_dim"):
            dim_str += "%s_%d-" % (k, args[k])
    if len(dim_str) > 0:
        dim_str = dim_str[:-1]

    # Append learning rate only for the SGD case
    args.lrate = float(args.lrate)
    opt_string = args.optimizer
    if args.optimizer == "sgd":
        opt_string += "-lr_%.4f" % args.lrate

    # Set model name
    name = "%s-%s-%s-bs_%d-valid_%s" % (args.model_type, dim_str, opt_string, args.batch_size, args.valid_metric)
    if args.decay_c > 0:
        name += "-decay_%.5f" % args.decay_c
    if args.clip_c > 0:
        name += "-gclip_%.1f" % args.clip_c
    if args.weight_init != 0.01:
        if isinstance(args.weight_init, str):
            p = args.weight_init
        else:
            p = "%.3f" % p
        name += "-weights_%s" % p
    if args.dropout > 0:
        name += "-dropout_%.1f" % args.dropout
    if args.seed != 1234:
        name += "-seed_%d" % args.seed

    if len(args.get('suffix', '')) > 0:
        name = "%s-%s" % (name, args.suffix)

    if 'suffix' in args:
        del args['suffix']

    args.model_path = os.path.join(args.model_path, args.model_path_suffix)
    del args['model_path_suffix']

    ensure_dirs([args.model_path])
    args.model_path = os.path.join(args.model_path, name + ".npz")

    return args

##########
# main() #
##########
if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='nmt-train')

    parser.add_argument('-c', '--config'        , type=str,   help="Path to model configuration file.", required=True)
    parser.add_argument('-s', '--suffix'        , type=str,   help="Optional suffix to append to model name.")

    parser.add_argument('-d', '--dropout'       , type=float, help="Dropout ratio")
    parser.add_argument('-w', '--decay-c'       , type=float, help="Weight decay factor")
    parser.add_argument('-g', '--clip-c'        , type=float, help="Gradient clipping factor")
    parser.add_argument('-b', '--batch-size'    , type=int  , help="Training batch size")
    parser.add_argument('-V', '--valid-freq'    , type=int  , help="Validation frequency in terms of updates")
    parser.add_argument('-M', '--valid-metric'  , type=str  , help="Validation metric: px, bleu, meteor")
    parser.add_argument('-R', '--seed'          , type=int  , help="Random seed")
    parser.add_argument('-o', '--optimizer'     , type=str  , help="Optimizer: sgd, adadelta, rmsprop, adam")
    parser.add_argument('-l', '--lrate'         , type=str  , help="Learning rate for SGD")
    parser.add_argument('-j', '--njobs'         , type=str  , help="# of CPU jobs for beam-search")
    parser.add_argument('-D', '--device-id'     , type=str  , help="Device to use")
    parser.add_argument('-N', '--nanguard'      , default=argparse.SUPPRESS, action='store_true', help="Debug NaN values")
    pargs = parser.parse_args()

    # First parse configuration file for arguments
    args = Config(pargs.config).get()
    args['model_path_suffix'] = os.path.splitext(os.path.basename(pargs.config))[0]
    del pargs.__dict__['config']

    # Append user-defined suffix to model id
    if pargs.suffix:
        args['suffix'] = pargs.suffix

    # Update args by reading cmdline args
    args.update([(k,v) for k,v in pargs.__dict__.items() if v is not None])

    # Bring default values for arguments which are not set
    all_defaults = dict(DEFAULTS)
    all_defaults.update(TRAIN_DEFAULTS)
    for k,v in all_defaults.iteritems():
        if k not in args:
            args[k] = v

    ###################
    # Setup arguments
    ###################
    args = setup(args)

    ######################################################
    # Start logging module (both to terminal and to file)
    ######################################################
    log_file = os.path.join(args.model_path.replace(".npz", ".log"))
    Logger.set_file(log_file)
    log = Logger._logger

    #############
    # Reserve GPU
    #############
    args.device_id = get_gpu(args.device_id)
    os.environ["THEANO_FLAGS"] = "device=%s" % args.device_id

    import theano
    import theano.tensor as tensor
    log.info("Using device: %s (on machine %s)" % (args.device_id, platform.node()))

    ##################
    # Dump parameters
    ##################
    log.info("Model options:")
    log.info("**************")
    for p in sorted(args.keys()):
        log.info(" %20s -> %s" % (p, args[p]))

    ########
    # Seed #
    ########
    # Don't seed numpy if seed == 0
    # But we need to seed theano.
    if args.seed == 0:
        args.seed = 1234
    else:
        # Set numpy random seed before everything else
        np.random.seed(args.seed)

    #############################################
    # Import the class from between nmtpy.models
    #############################################
    try:
       Model = importlib.import_module("nmtpy.models.%s" % args.model_type).Model
    except ImportError as e:
        log.error("Error while importing %s" % args.model_type)
        log.error(e)
        sys.exit(1)

    # Create model object
    # Pass parameters not in TRAIN_DEFAULTS as they are for this mainloop
    model_args = dict([(k,v) for k,v in args.items() if k not in TRAIN_DEFAULTS])
    model = Model(seed=args.seed, **model_args)

    if model.options.get('n_words_src', 0) > 0:
        log.info("Source vocabulary size: %d" % model.options['n_words_src'])
    if model.options.get('n_words_trg', 0) > 0:
        log.info("Target vocabulary size: %d" % model.options['n_words_trg'])

    log.info("Preparing data")
    model.load_data()
    log.info("Training data: %d samples" % model.train_iterator.n_samples)
    do_valid = False
    if model.valid_iterator:
        log.info("Validation data: %d samples" % model.valid_iterator.n_samples)
        do_valid = True

    # If valid_freq == 0, do validation at end of epochs
    if args.valid_freq == 0:
        args.valid_freq = math.ceil(model.train_iterator.n_samples / float(args.batch_size))

    log.info("Early stopping metric is %s" % args.valid_metric)

    # Initialize parameters
    log.info("Initializing parameters")
    model.init_params()
    model.init_shared_variables()

    # Build the model
    log.info("Building model")
    cost = model.build()

    log.info("Input tensor order: ")
    log.info(model.inputs.values())

    log.info('Building sampler')
    model.build_sampler()

    # apply L2 regularization on weights
    if args.decay_c > 0.:
        cost = model.add_l2_weight_decay(cost, args.decay_c)

    # This is a NO-OP if not reimplemented in the model
    if args.alpha_c > 0.:
        cost = model.add_alpha_regularizer(cost, args.alpha_c)

    # Build optimizer
    log.info('Building "%s" optimizer' % args.optimizer)
    model.build_optimizer(cost, args.clip_c)

    # Save model options
    model.save_options()

    early_stop      = False
    uidx            = 0
    bad_counter     = 0
    valid_halved    = False
    valid_losses    = []
    metric_history  = []
    metric_results  = []
    cur_lrate       = args.lrate

    # Helper method for summary
    def dump_val_summary():
        if len(metric_history) > 0:
            best_metric_idx = np.argmax(np.array(metric_history))
            best_valid = metric_results[best_metric_idx]
            for _, v in sorted(best_valid.iteritems()):
                log.info("[Validation %3d] Best %s" % (best_metric_idx + 1, v[0]))
        else:
            best_valid_idx = np.argmin(np.array(valid_losses))
            log.info('[Validation %3d] Best loss %5.5f' % (best_valid_idx + 1, valid_losses[best_valid_idx]))

    ################
    # Training loop
    ################
    for eidx in xrange(1, args.max_epochs + 1):
        msg = 'Starting epoch %d' % eidx
        log.info(msg)
        log.info('-' * len(msg))

        # Train error list
        train_losses = []

        for batch_dict in model.train_iterator:
            uidx += 1

            # Enable dropout in training (if requested)
            model.set_dropout(True)

            # compute cost, grads and copy grads to shared variables
            ud_start = time.time()
            train_losses.append(model.f_grad_shared(*batch_dict.values()))
            if np.isnan(train_losses[-1]):
                raise Exception("NaN in cost")
            model.f_update(cur_lrate)
            ud = time.time() - ud_start

            # verbose
            if uidx % 10 == 0:
                log.info("Epoch: %4d, update: %7d, Cost: %10.2f, Time: %.3f" % (eidx, uidx, train_losses[-1], ud))

            # sample?
            if args.sample_freq > 0 and uidx % args.sample_freq == 0:
                samples = model.generate_samples(batch_dict, n_samples=5)
                if samples is not None:
                    for src, truth, sample in samples:
                        if src:
                            log.info("Source: %s" % src)
                        log.info(" Truth: %s" % truth)
                        log.info("Sample: %s" % sample)

           ###############################################################
            # Validate model on validation set and early stop if necessary
            ###############################################################
            if do_valid and eidx >= args.valid_start and uidx % args.valid_freq == 0:
                # Compute validation loss
                model.set_dropout(False)
                valid_losses.append(model.val_loss())

                if np.isnan(valid_losses[-1]):
                    raise Exception("NaN in validation loss")

                # Start beam-search if validation metric is different from NLL/PX
                if args.valid_metric != 'px':
                    t = time.time()
                    log.info("Starting translation")
                    results = model.run_beam_search(beam_size=12,
                                                    n_jobs=args.njobs,
                                                    metric=args.valid_metric,
                                                    mode=args.decoder_mode)
                    log.info("Done: %.2f seconds" % (time.time() - t))

                    # We'll receive the requested metrics in a dict
                    metric_results.append(results)
                    for _, v in sorted(results.iteritems()):
                        log.info("[Validation %2d] %s" % (len(metric_history)+1, v[0]))
                    log.info("[Validation %2d] LOSS = %5.5f" % (len(valid_losses), valid_losses[-1]))

                    # Pick the one selected as valid_metric and add it to metric_history
                    metric_history.append(results[args.valid_metric][1])

                # Is this the best model?
                # Also save if we have no models saved so far.
                best = (len(metric_history) == 1)
                if len(metric_history) > 1 and metric_history[-1] > np.array(metric_history[:-1]).max():
                    best = True
                elif len(valid_losses) > 1 and args.valid_metric == 'px' and \
                        valid_losses[-1] < np.array(valid_losses[:-1]).min():
                    best = True

                if best:
                    log.info('Saving the best model')
                    bad_counter = 0

                    # Save the best model or the current model if no best model is found so far
                    model.save_params(args.model_path,
                                      valid_losses=valid_losses,
                                      metric_history=metric_history,
                                      uidx=uidx, **unzip(model.tparams))

                    # save with uidx as well
                    if args.save_iter:
                        log.info('Saving the model at iteration %d' % (uidx))
                        model_path_uidx = '%s.iter%d.npz' % (os.path.splitext(args.model_path)[0], uidx)
                        shutil.copy(args.model_path, model_path_uidx)

                elif len(valid_losses) > args.patience:
                    # We passed the limit for checking patience
                    is_bad = False
                    if args.valid_metric == 'px':
                        if valid_losses[-1] >= np.array(valid_losses)[-args.patience:].min():
                            is_bad = True
                    else:
                        if metric_history[-1] <= np.array(metric_history)[-args.patience:].max():
                            is_bad = True

                    if is_bad:
                        #if eidx >= args.valid_freq_decay and not valid_halved and bad_counter == 0:
                        #    args.valid_freq = int(args.valid_freq / 2)
                        #    valid_halved = True
                        #    log.info("Validation frequency halved: %d" % args.valid_freq)
                        bad_counter += 1
                        log.info("No validation improvement, bad-counter = %d (patience %d)" % (bad_counter, args.patience))

                # Remember who we are
                msg = "Model is --> %s" % model.name
                log.info('-' * len(msg))
                log.info(msg)
                dump_val_summary()

                if bad_counter > args.patience:
                    early_stop = True
                    break

            # finish after this many updates
            if uidx >= args.max_iteration:
                log.info('Training stopped after %d iterations.' % uidx)
                early_stop = True
                break

        # Epoch finished
        train_losses = np.array(train_losses)
        log.info("Epoch %d finished with mean batch loss: %.3f" % (eidx, train_losses.mean()))

        if early_stop:
            log.info('Early Stopped.')
            break

    ################################
    # Dump final validation results
    ################################
    dump_val_summary()
    sys.exit(0)

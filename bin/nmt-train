#!/usr/bin/env python
import os
import sys
import logging
import argparse
import importlib
import platform

from nmtpy.config import Config
from nmtpy.logger import Logger
from nmtpy.sysutils import setup_train_args, get_gpu
from nmtpy.nmtutils import DotDict
from nmtpy.mainloop import MainLoop

# Ensure cleaning up temp files and processes
import nmtpy.cleanup as cleanup
cleanup.register_handler()

DEFAULTS = {
        'weight_init':        0.01,           # Scale of the uniform distribution for weight initialization.
                                              # Can be a float, "xavier" or "he".
        'batch_size':         32,             # Training batch size
        'lrate':              0.0001,         # Initial learning rate
        'optimizer':          'adadelta',     # adadelta, sgd, rmsprop, adam
        }

TRAIN_DEFAULTS = {
        'debug':              False,          # Dump graph and print theano node input/outputs
        'decay_c':            0.0005,         # L2 penalty factor
        'clip_c':             5.,             # Clip gradients above clip_c
        'alpha_c':            0.,             # Alpha regularization for attentional models
        'seed':               1234,           # RNG seed
        'suffix':             "",             # Model suffix
        'save_iter':          False,          # Save each best valid weights to separate file
        'device_id':          'auto',         #
        'patience':           10,             # Early stopping patience
        'max_epochs':         100,            # Max number of epochs to train
        'max_iteration':      int(1e6),       # Max number of updates to train
        'valid_start':        1,              # Epoch which validation will start
        'valid_freq':         0,              # 0: End of epochs
        'valid_metric':       'bleu',         # bleu, px, meteor
        'sample_freq':        0,              # Sampling frequency during training (0: disabled)
        'njobs':              10,             # # of parallel CPU tasks to do beam-search
        'beam_size':          12,             # Allow changing beam size during validation
        }

# Bring default values for arguments which are not set
all_defaults = dict(DEFAULTS)
all_defaults.update(TRAIN_DEFAULTS)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='nmt-train')

    parser.add_argument('-D', '--device-id'     , type=str  , help="Device to use")
    parser.add_argument('-M', '--valid-metric'  , type=str  , help="Validation metric: px, bleu, meteor")
    parser.add_argument('-R', '--seed'          , type=int  , help="Random seed")
    parser.add_argument('-T', '--model-type'    , type=str,   help="Override the model type given in the configuration file.")
    parser.add_argument('-V', '--valid-freq'    , type=int  , help="Validation frequency in terms of updates")
    parser.add_argument('-a', '--alpha-c'       , type=float, help="Alpha regularization factor")
    parser.add_argument('-b', '--batch-size'    , type=int  , help="Training batch size")
    parser.add_argument('-c', '--config'        , type=str,   help="Path to model configuration file.", required=True)
    parser.add_argument('-d', '--debug'         , action='store_true', help="Enable debugging outputs (Warning: very verbose)")
    parser.add_argument('-e', '--extra'         , nargs='*' , help="Extra key:value pairs as in conf to override them.")
    parser.add_argument('-g', '--clip-c'        , type=float, help="Gradient clipping factor")
    parser.add_argument('-j', '--njobs'         , type=str  , help="# of CPU jobs for beam-search")
    parser.add_argument('-l', '--lrate'         , type=str  , help="Initial learning rate")
    parser.add_argument('-o', '--optimizer'     , type=str  , help="Optimizer: sgd, adadelta, rmsprop, adam")
    parser.add_argument('-s', '--suffix'        , type=str,   help="Optional suffix to append to model name.")
    parser.add_argument('-w', '--decay-c'       , type=float, help="Weight decay factor")

    pargs = parser.parse_args()

    # First parse configuration file for arguments as base
    args = Config(pargs.config).get()

    # Create a folder named as conf file
    args['model_path_suffix'] = os.path.splitext(os.path.basename(pargs.config))[0]
    extra_params = pargs.extra
    del pargs.__dict__['config']
    del pargs.__dict__['extra']

    # Override .conf arguments from the ones given in cmdline
    args.update([(k,v) for k,v in pargs.__dict__.items() if v is not None])

    for k,v in all_defaults.iteritems():
        if k not in args:
            args[k] = v

    if extra_params:
        for opt in extra_params:
            key, value = opt.split(":")
            key = key.replace('-', '_')
            args[key] = eval(value, {}, {})

    # Setup arguments
    args, log_file = setup_train_args(args)
    # Pass parameters not in TRAIN_DEFAULTS as they are for this mainloop
    model_args = dict([(k,v) for k,v in args.iteritems() if k not in TRAIN_DEFAULTS])
    train_args = dict([(k,v) for k,v in args.iteritems() if k in TRAIN_DEFAULTS])
    train_args = DotDict(train_args)

    # Start logging module (both to terminal and to file)
    Logger.set_file(log_file)
    log = Logger._logger

    # Reserve GPU
    if 'THEANO_FLAGS' not in os.environ:
        args.device_id = get_gpu(args.device_id)
        os.environ['THEANO_FLAGS'] = "device=%s" % args.device_id

    log.info("THEANO_FLAGS = %s" % os.environ['THEANO_FLAGS'])

    # Import theano
    import theano
    import numpy as np
    log.info("Using device: %s (on machine %s)" % (args.device_id, platform.node()))

    # Set numpy random seed before everything else
    if args.seed != 0:
        np.random.seed(args.seed)

    # Import the model
    try:
       Model = importlib.import_module("nmtpy.models.%s" % args.model_type).Model
    except ImportError as e:
        log.error("Error while importing %s" % args.model_type)
        log.error(e)
        sys.exit(1)

    # Print parameter summary
    log.info("Model options:")
    log.info("--------------")
    for p in sorted(args.keys()):
        log.info(" %20s -> %s" % (p, args[p]))

    # Create model object
    model = Model(seed=args.seed, logger=log, **model_args)

    # Initialize parameters
    log.info("Initializing parameters")
    model.init_params()
    model.init_shared_variables()
    log.info("Number of parameters: %s" % model.get_nb_params())

    # Load data
    log.info("Loading data")
    model.load_data()

    # Dump model information
    model.info()

    # Build the model
    log.info("Building model")
    cost = model.build()

    log.info("Input tensor order: ")
    log.info(model.inputs.values())

    if args.sample_freq > 0:
        log.info('Building sampler')
        model.build_sampler()

    # Get regularized training cost
    cost = model.get_regularized_cost(cost, args.decay_c, args.alpha_c)

    # Build optimizer
    log.info('Initial learning rate: %.5f' % model.lrate)
    log.info('Building optimizer %s' % args.optimizer)
    model.build_optimizer(cost, args.clip_c, debug=args.debug)

    # Save graph
    if args.debug:
        theano.printing.debugprint(model.train_batch,
                                   file=open('%s.graph' % log_file.replace(".log", ""), 'w'))

    # Reseed to retain the order of shuffle operations
    np.random.seed(args.seed)

    # Create mainloop
    loop = MainLoop(model, log, train_args)
    loop.run()
